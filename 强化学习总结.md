
##  学习资料总结

[TOC]

### 学习流派

DeepMind：Alphago的幕后团队，是Google的。主要在于Model Free的预测，类似于基于Value和基于Policy的强化学习

OpenAI：一个非营利的人工智能研究组织。主要在于有模型的控制。

###  大学课程

1、斯坦福大学的《强化学习》

老师 David Silver，DeepMind

课程链接：   <http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html>

课程听起来思维很跳跃，跟不上老师的思维。

2、UC Berkeley的CS 294-112，Deep Reinforcement Learning

深度强化学习，OpenAI

课程上很多数学理论推导，讲解的很详细，就是数学公式推导很难理解。

3、台湾 李宏毅的深度强化学习

这个比较容易理解，非常适合学习。

###  其他参考

1、莫烦的强化学习课程

简单易懂，适合入门，纯粹科普，没有数学公式推导

2、博客园上刘建平的系列

<https://www.cnblogs.com/pinard/p/9385570.html>

比莫烦多了一些理论推导，但是课程讲解内容偏少。

3、知乎上的 《强化学习大讲堂》 专栏：

<https://zhuanlan.zhihu.com/sharerl>

这个讲的很详细。

## 开源库总结

###  游戏环境

1、Control Suite
Google的DeepMind团队提供，类似于openAI的gym，提供了很多强化学习环境

2、roboschool
OpenAI开源的，机器人环境

###   算法实现

1、Dopamine
多巴胺，Google开源的，没太明白干啥的？提供了很多游戏环境
License：Apache

2、TRFL
松露，Google的DeepMind团队提供，强化学习开源库，可以很容易实现DQN、Policy Gradient等算法
License：Apache

3、Ray/RLlib
openAI提供的，强化学习分布式任务处理
License：Apache

4、ChainerRL
implements various state-of-the-art deep reinforcement algorithms in Python，常用的强化学习算法都有。
License：MIT

5、OpenAI Baselines
OpenAI Baselines is a set of high-quality implementations of reinforcement learning algorithms
License：MIT

6、Coach
Intel开源的强化学习库
License：Apache

7、TensorForce
Tensorflow提供的强化学习API
License：Apache

##  算法总结

1、DQN：
Q值的计算不是直接通过Q_Table来计算，而是通过深度神经网络来计算的

2、Nature DQN：
使用了两个Q网络，一个当前Q网络Q用来选择动作，更新模型参数，另一个目标Q网络Q′用于计算目标Q值。
目标Q网络的网络参数不需要迭代更新，而是每隔一段时间从当前Q网络Q复制过来，即延时更新，这样可以减少目标Q值和当前的Q值相关性。

3、Double DQN (DDQN)
解决了DQN的高方差问题
解耦目标Q值动作的选择和目标Q值的计算这两步。

4、Prioritized Replay DQN
在DQN基础上增加了样本回放的优先级，加快收敛速度

5、Dueling DQN
在DQN基础上改进了神经网络的结构

6、Multi-Step Learning
解决了DQN的高偏差问题
原始的DQN使用的是当前的即时奖励r和下一时刻的价值估计作为目标价值，这种方法在前期策略差即网络参数偏差较大的情况下，得到的目标价值偏差也较大，因此学习速度可能相对较慢。因此我们可以通过Multi-Step Learning来解决这个问题，这样在训练前期目标价值可以得到更准确的估计，从而加快训练速度。

7、Distributional DQN
在DQN中，网络输出的都是状态-动作价值Q的期望预估值。比如同一状态下的两个动作，能够获得的价值期望是相同的，比如都是20，第一个动作在90%的情况下价值是10，在10%的情况下是110，另一个动作在50%的情况下是15，在50%的情况下是25。那么虽然期望一样，但如果我们想要减小风险，我们应该选择后一种动作。而只有期望值的话，我们是无法看到动作背后所蕴含的风险的。
所以从理论上来说，从分布视角（distributional perspective）来建模我们的深度强化学习模型，可以获得更多有用的信息，从而得到更好、更稳定的结果。

8、NoisyNet
该方法通过对参数增加噪声来增加模型的探索能力。

RainBow融合了这些所有方法。从Rainbow里面单独去掉每种方法后如下图：Double没啥用，因为Double解决的是高方差问题，而Distributional后一般不会出现高方差，只会出现高偏差。







